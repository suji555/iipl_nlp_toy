{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Example2 | PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 0. GPU Setting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "source": [
    "## 1. Data load"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
    "test = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00002165364db923c7e6</td>\n      <td>How did Quebec nationalists see their province...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000032939017120e6e44</td>\n      <td>Do you have an adopted dog, how would you enco...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000412ca6e4628ce2cf</td>\n      <td>Why does velocity affect time? Does velocity a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000042bf85aa498cd78e</td>\n      <td>How did Otto von Guericke used the Magdeburg h...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000455dfa3e01eae3af</td>\n      <td>Can I convert montra helicon D to a mountain b...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:30000]"
   ]
  },
  {
   "source": [
    "## 2. Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def alpha_num(text):\n",
    "    return re.sub(r'[^a-zA-z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = train['question_text'].str.lower().apply(alpha_num)"
   ]
  },
  {
   "source": [
    "## 3. Train & validation split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "valid_percent = 0.2\n",
    "\n",
    "data_len = len(train)\n",
    "valid_index = np.random.choice(range(data_len), int(data_len*valid_percent), replace=False)\n",
    "train_index = list(set(range(data_len)) - set(valid_index))\n",
    "\n",
    "train_text_list = [text_list[i] for i in train_index]\n",
    "valid_text_list = [text_list[i] for i in valid_index]\n",
    "train_label_list = [train['target'].tolist()[i] for i in train_index]\n",
    "valid_label_list = [train['target'].tolist()[i] for i in valid_index]"
   ]
  },
  {
   "source": [
    "## 4. Parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = './save'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "vocab_size = 12000\n",
    "pad_idx = 0\n",
    "bos_idx = 1\n",
    "eos_idx = 2\n",
    "unk_idx = 3\n",
    "\n",
    "# 1) Make Korean text to train vocab\n",
    "with open(f'{save_path}/text.txt', 'w') as f:\n",
    "    for text in train_text_list:\n",
    "        f.write(f'{text}\\n')\n",
    "\n",
    "\n",
    "# 2) SentencePiece model training\n",
    "spm.SentencePieceProcessor()\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f'--input={save_path}/text.txt --model_prefix={save_path}/m_text '\n",
    "    f'--vocab_size={vocab_size} --character_coverage=0.9995 '\n",
    "    f'--model_type=bpe --split_by_whitespace=true '\n",
    "    f'--pad_id={pad_idx} --unk_id={unk_idx} '\n",
    "    f'--bos_id={bos_idx} --eos_id={eos_idx}'\n",
    ")\n",
    "\n",
    "vocab_list = list()\n",
    "with open(f'{save_path}/m_text.vocab') as f:\n",
    "    for line in f:\n",
    "        vocab_list.append(line[:-1].split('\\t')[0])\n",
    "word2id_spm = {w: i for i, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece model load\n",
    "spm_ = spm.SentencePieceProcessor()\n",
    "spm_.Load(f\"{save_path}/m_text.model\")\n",
    "\n",
    "# Tokenizing\n",
    "train_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in train_text_list]\n",
    "valid_encoded_list = [[bos_idx] + spm_.EncodeAsIds(text) + [eos_idx] for text in valid_text_list]"
   ]
  },
  {
   "source": [
    "## 5. Custom dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_list, trg_list, min_len=4, max_len=500):\n",
    "        data = list()\n",
    "        for src, trg in zip(src_list, trg_list):\n",
    "            if min_len <= len(src) <= max_len:\n",
    "                data.append((src, trg))\n",
    "\n",
    "        self.data = data\n",
    "        self.num_data = len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src, trg = self.data[index]\n",
    "        return src, trg\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "class PadCollate:\n",
    "    def __init__(self, pad_index=0, dim=0):\n",
    "        self.dim = dim\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        def pad_tensor(vec, max_len, dim):\n",
    "            pad_size = list(vec.shape)\n",
    "            pad_size[dim] = max_len - vec.size(dim)\n",
    "            return torch.cat([vec, torch.LongTensor(*pad_size).fill_(self.pad_index)], dim=dim)\n",
    "\n",
    "        def pack_sentence(sentences):\n",
    "            sentences_len = max(map(lambda x: len(x), sentences))\n",
    "            sentences = [pad_tensor(torch.LongTensor(seq), sentences_len, self.dim) for seq in sentences]\n",
    "            sentences = torch.cat(sentences)\n",
    "            sentences = sentences.view(-1, sentences_len)\n",
    "            return sentences\n",
    "\n",
    "        src, trg = zip(*batch)\n",
    "        return pack_sentence(src), torch.LongTensor(trg)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "source": [
    "## 6. DataLoader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of trainingsets  iterations - 24000, 3000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "dataset_dict = {\n",
    "    'train': CustomDataset(train_encoded_list, train_label_list, min_len=4, max_len=500),\n",
    "    'valid': CustomDataset(valid_encoded_list, valid_label_list, min_len=4, max_len=500)\n",
    "}\n",
    "\n",
    "dataloader_dict = {\n",
    "    'train': DataLoader(dataset_dict['train'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=batch_size, num_workers=4, shuffle=True, pin_memory=True),\n",
    "    'valid': DataLoader(dataset_dict['valid'], collate_fn=PadCollate(), drop_last=True,\n",
    "                        batch_size=batch_size, num_workers=4, shuffle=True, pin_memory=True)\n",
    "}\n",
    "\n",
    "print(f'Total number of trainingsets  iterations - {len(dataset_dict[\"train\"])}, {len(dataloader_dict[\"train\"])}')"
   ]
  },
  {
   "source": [
    "## 7. Build model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, src_vocab_num, trg_num=2, pad_idx=0, bos_idx=1, eos_idx=2, \n",
    "                 max_len=500, d_model=512, d_embedding=256, dropout=0.1):\n",
    "\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_num, d_embedding, padding_idx=pad_idx)\n",
    "        self.linear1 = nn.Linear(d_embedding, d_model)\n",
    "        self.linear2 = nn.Linear(d_model, trg_num)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        embedding_out = self.src_embedding(src)\n",
    "        gap_out = embedding_out.mean(dim=1)\n",
    "        linear_out = self.dropout(F.gelu(self.linear1(gap_out)))\n",
    "        logit = self.linear2(linear_out)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (src_embedding): Embedding(12000, 256, padding_idx=0)\n",
       "  (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomModel(vocab_size, trg_num=2, pad_idx=pad_idx, bos_idx=bos_idx, eos_idx=eos_idx,\n",
    "                    max_len=500, d_model=512, d_embedding=256, dropout=0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "source": [
    "## 8. Optimizer setting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "lr = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, \n",
    "                              patience=len(dataloader_dict['train'])/1.5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "source": [
    "## 9. Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Fitting: [1/5]\n",
      "[Epoch:1][0/3000] train_loss:0.679 | Accuracy:0.625 | lr:0.010000 | spend_time: 0.00min\n",
      "[Epoch:1][1500/3000] train_loss:0.515 | Accuracy:0.750 | lr:0.010000 | spend_time: 0.08min\n",
      "[Epoch:1] val_loss:0.192 | Accuracy: 0.94 | spend_time: 0.18min\n",
      "[!] saving model...\n",
      "Model Fitting: [2/5]\n",
      "[Epoch:2][0/3000] train_loss:0.068 | Accuracy:1.000 | lr:0.010000 | spend_time: 0.00min\n",
      "[Epoch:2][1500/3000] train_loss:0.400 | Accuracy:0.750 | lr:0.010000 | spend_time: 0.08min\n",
      "[Epoch:2] val_loss:0.195 | Accuracy: 0.94 | spend_time: 0.19min\n",
      "Model Fitting: [3/5]\n",
      "[Epoch:3][0/3000] train_loss:0.024 | Accuracy:1.000 | lr:0.010000 | spend_time: 0.00min\n",
      "[Epoch:3][1500/3000] train_loss:0.163 | Accuracy:0.875 | lr:0.001000 | spend_time: 0.08min\n",
      "[Epoch:3] val_loss:0.190 | Accuracy: 0.94 | spend_time: 0.18min\n",
      "[!] saving model...\n",
      "Model Fitting: [4/5]\n",
      "[Epoch:4][0/3000] train_loss:0.530 | Accuracy:0.875 | lr:0.001000 | spend_time: 0.00min\n",
      "[Epoch:4][1500/3000] train_loss:0.159 | Accuracy:1.000 | lr:0.000100 | spend_time: 0.08min\n",
      "[Epoch:4] val_loss:0.190 | Accuracy: 0.94 | spend_time: 0.18min\n",
      "Model Fitting: [5/5]\n",
      "[Epoch:5][0/3000] train_loss:0.462 | Accuracy:0.750 | lr:0.000010 | spend_time: 0.00min\n",
      "[Epoch:5][1500/3000] train_loss:0.036 | Accuracy:1.000 | lr:0.000001 | spend_time: 0.08min\n",
      "[Epoch:5] val_loss:0.190 | Accuracy: 0.94 | spend_time: 0.19min\n",
      "[!] saving model...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "num_epoch = 5\n",
    "\n",
    "print_freq = 1500\n",
    "best_val_loss = None\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    start_time_e = time.time()\n",
    "    print(f'Model Fitting: [{e+1}/{num_epoch}]')\n",
    "    for phase in ['train', 'valid']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "            freq = 0\n",
    "        if phase == 'valid':\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_acc = 0\n",
    "\n",
    "        for i, (src, trg) in enumerate(dataloader_dict[phase]):\n",
    "            # Optimizer setting\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Source, Target sentence setting\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            # Model / Calculate loss\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                predicted_logit = model(src)\n",
    "                # If phase train, then backward loss and step optimizer and scheduler\n",
    "                if phase == 'train':\n",
    "                    loss = criterion(predicted_logit, trg)\n",
    "                    loss.backward()\n",
    "                    clip_grad_norm_(model.parameters(), 5)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step(loss)\n",
    "                    # Print loss value only training\n",
    "                    if freq == print_freq or freq == 0 or i == len(dataloader_dict['train']):\n",
    "                        total_loss = loss.item()\n",
    "                        _, predicted = predicted_logit.max(dim=1)\n",
    "                        accuracy = sum(predicted == trg).item() / predicted.size(0)\n",
    "                        print(\"[Epoch:%d][%d/%d] train_loss:%5.3f | Accuracy:%2.3f | lr:%1.6f | spend_time:%5.2fmin\"\n",
    "                                % (e+1, i, len(dataloader_dict['train']), total_loss, accuracy, \n",
    "                                optimizer.param_groups[0]['lr'], (time.time() - start_time_e) / 60))\n",
    "                        freq = 0\n",
    "                    freq += 1\n",
    "                if phase == 'valid':\n",
    "                    loss = F.cross_entropy(predicted_logit, trg)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = predicted_logit.max(dim=1)\n",
    "                    val_acc += sum(predicted == trg).item() / predicted.size(0)\n",
    "        # Finishing iteration\n",
    "        if phase == 'valid':\n",
    "            val_loss /= len(dataloader_dict['valid'])\n",
    "            val_acc /= len(dataloader_dict['valid'])\n",
    "            print(\"[Epoch:%d] val_loss:%5.3f | Accuracy:%5.2f | spend_time:%5.2fmin\"\n",
    "                    % (e+1, val_loss, val_acc, (time.time() - start_time_e) / 60))\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                print(\"[!] saving model...\")\n",
    "                torch.save(model.state_dict(), \n",
    "                            os.path.join(save_path, f'model_saved.pt'))\n",
    "                best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
